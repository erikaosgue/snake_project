I210118 05:38:18.644090 1 util/log/sync_buffer.go:195 ⋮ [config] file created at: 2021/01/18 05:38:18
I210118 05:38:18.644098 1 util/log/sync_buffer.go:195 ⋮ [config] running on machine: ‹TK-Pad›
I210118 05:38:18.644105 1 util/log/sync_buffer.go:195 ⋮ [config] binary: CockroachDB CCL v20.2.3 (x86_64-unknown-linux-gnu, built 2020/12/14 18:33:39, go1.13.14)
I210118 05:38:18.644112 1 util/log/sync_buffer.go:195 ⋮ [config] arguments: ‹[cockroach start-single-node --insecure --listen-addr=localhost:26257 --http-addr=localhost:8080]›
I210118 05:38:18.644121 1 util/log/sync_buffer.go:195 ⋮ [config] line format: [IWEF]yymmdd hh:mm:ss.uuuuuu goid file:line msg utf8=✓
W210118 05:38:18.643992 1 cli/start.go:1139 ⋮ ALL SECURITY CONTROLS HAVE BEEN DISABLED!

This mode is intended for non-production testing only.

In this mode:
- Your cluster is open to any client that can access ‹localhost›.
- Intruders with access to your machine or network can observe client-server traffic.
- Intruders can log in without password and read or write any data in the cluster.
- Intruders can consume all your server's resources and cause unavailability.
I210118 05:38:18.644201 1 cli/start.go:1149 ⋮ To start a secure server without mandating TLS for clients,
consider --accept-sql-without-tls instead. For other options, see:

- ‹https://go.crdb.dev/issue-v/53404/v20.2›
- https://www.cockroachlabs.com/docs/v20.2/secure-a-cluster.html
I210118 05:38:18.644494 1 server/status/recorder.go:605 ⋮ ‹available memory from cgroups is unsupported, using system memory 15 GiB instead: can't read available memory from cgroup v2 at /sys/fs/cgroup/unified/user.slice/user-1000.slice/user@1000.service/memory.max: open /sys/fs/cgroup/unified/user.slice/user-1000.slice/user@1000.service/memory.max: no such file or directory›
W210118 05:38:18.644510 1 cli/start.go:983 ⋮ ‹Using the default setting for --cache (128 MiB).›
‹  A significantly larger value is usually needed for good performance.›
‹  If you have a dedicated server a reasonable setting is --cache=.25 (3.8 GiB).›
I210118 05:38:18.644700 1 server/status/recorder.go:605 ⋮ ‹available memory from cgroups is unsupported, using system memory 15 GiB instead: can't read available memory from cgroup v2 at /sys/fs/cgroup/unified/user.slice/user-1000.slice/user@1000.service/memory.max: open /sys/fs/cgroup/unified/user.slice/user-1000.slice/user@1000.service/memory.max: no such file or directory›
I210118 05:38:18.644711 1 cli/start.go:1164 ⋮ ‹CockroachDB CCL v20.2.3 (x86_64-unknown-linux-gnu, built 2020/12/14 18:33:39, go1.13.14)›
I210118 05:38:18.645163 1 server/status/recorder.go:605 ⋮ ‹available memory from cgroups is unsupported, using system memory 15 GiB instead: can't read available memory from cgroup v2 at /sys/fs/cgroup/unified/user.slice/user-1000.slice/user@1000.service/memory.max: open /sys/fs/cgroup/unified/user.slice/user-1000.slice/user@1000.service/memory.max: no such file or directory›
I210118 05:38:18.645174 1 server/config.go:434 ⋮ system total memory: ‹15 GiB›
I210118 05:38:18.645186 1 server/config.go:436 ⋮ server configuration:
‹max offset             500000000›
‹cache size             128 MiB›
‹SQL memory pool size   3.8 GiB›
‹scan interval          10m0s›
‹scan min idle time     10ms›
‹scan max idle time     1s›
‹event log enabled      true›
I210118 05:38:18.645222 1 cli/start.go:961 ⋮ using local environment variables: ‹COCKROACH_BACKGROUND_RESTART=1›
I210118 05:38:18.645231 1 cli/start.go:968 ⋮ process identity: ‹uid 1000 euid 1000 gid 1000 egid 1000›
I210118 05:38:18.648381 1 cli/start.go:504 ⋮ GEOS loaded from directory ‹/usr/local/lib/cockroach›
I210118 05:38:18.648420 1 cli/start.go:509 ⋮ starting cockroach node
I210118 05:38:18.665737 23 server/server.go:782 ⋮ [n?] monitoring forward clock jumps based on server.clock.forward_jump_check_enabled
I210118 05:38:18.736372 23 server/config.go:625 ⋮ [n?] 1 storage engine‹› initialized
I210118 05:38:18.736415 23 server/config.go:628 ⋮ [n?] ‹Pebble cache size: 128 MiB›
I210118 05:38:18.736430 23 server/config.go:628 ⋮ [n?] ‹store 0: RocksDB, max size 0 B, max open file limit 10000›
I210118 05:38:18.738692 200 server/server.go:1416 ⋮ [n?] connecting to gossip network to verify cluster ID ‹"55e6c439-0264-4579-bb6d-df06d1461430"›
I210118 05:38:18.740112 23 gossip/gossip.go:403 ⋮ [n1] NodeDescriptor set to ‹node_id:1 address:<network_field:"tcp" address_field:"localhost:26257" > attrs:<> locality:<> ServerVersion:<major_val:20 minor_val:2 patch:0 unstable:0 > build_tag:"v20.2.3" started_at:1610948298740105553 cluster_name:"" sql_address:<network_field:"tcp" address_field:"localhost:26257" >›
I210118 05:38:18.746743 200 server/server.go:1419 ⋮ [n1] node connected via gossip
W210118 05:38:18.744434 269 kv/kvserver/replica_range_lease.go:555 ⋮ [n1,s1,r6/1:‹/Table/{SystemCon…-11}›] can't determine lease status of (n1,s1):1 due to node liveness error: node not in the liveness table
(1) attached stack trace
  -- stack trace:
  | github.com/cockroachdb/cockroach/pkg/kv/kvserver.init
  | 	/go/src/github.com/cockroachdb/cockroach/pkg/kv/kvserver/node_liveness.go:45
  | runtime.doInit
  | 	/usr/local/go/src/runtime/proc.go:5228
  | runtime.doInit
  | 	/usr/local/go/src/runtime/proc.go:5223
  | runtime.doInit
  | 	/usr/local/go/src/runtime/proc.go:5223
  | runtime.doInit
  | 	/usr/local/go/src/runtime/proc.go:5223
  | runtime.doInit
  | 	/usr/local/go/src/runtime/proc.go:5223
  | runtime.main
  | 	/usr/local/go/src/runtime/proc.go:190
  | runtime.goexit
  | 	/usr/local/go/src/runtime/asm_amd64.s:1357
Wraps: (2) node not in the liveness table
Error types: (1) *withstack.withStack (2) *errutil.leafError
W210118 05:38:18.747856 269 kv/kvserver/store.go:1704 ⋮ [n1,s1,r6/1:‹/Table/{SystemCon…-11}›] could not gossip system config: ‹[NotLeaseHolderError] r6: replica (n1,s1):1 not lease holder; lease holder unknown›
(1) ‹[NotLeaseHolderError] r6: replica (n1,s1):1 not lease holder; lease holder unknown›
Error types: (1) *roachpb.NotLeaseHolderError
I210118 05:38:18.748369 23 server/node.go:430 ⋮ [n1] initialized store [n1,s1]: disk (capacity=329 GiB, available=277 GiB, used=106 MiB, logicalBytes=209 MiB), ranges=36, leases=2, queries=0.00, writes=0.00, bytesPerReplica={p10=0.00 p25=0.00 p50=0.00 p75=23546.00 p90=50343.00 pMax=218407712.00}, writesPerReplica={p10=0.00 p25=0.00 p50=0.00 p75=0.00 p90=0.00 pMax=0.00}
I210118 05:38:18.748542 23 kv/kvserver/stores.go:236 ⋮ [n1] read 0 node addresses from persistent storage
I210118 05:38:18.749770 23 server/node.go:489 ⋮ [n1] started with engine type ‹2›
I210118 05:38:18.749825 23 server/node.go:491 ⋮ [n1] started with attributes ‹[]›
I210118 05:38:18.749904 23 server/goroutinedumper/goroutinedumper.go:120 ⋮ [n1] writing goroutine dumps to ‹/home/erikaosgue/erika_work/goland/truora_project/snake_project/backend/cockroach-data/logs/goroutine_dump›
I210118 05:38:18.749936 23 server/heapprofiler/heapprofiler.go:49 ⋮ [n1] writing go heap profiles to ‹/home/erikaosgue/erika_work/goland/truora_project/snake_project/backend/cockroach-data/logs/heap_profiler› at least every 1h0m0s
I210118 05:38:18.749959 23 server/heapprofiler/cgoprofiler.go:53 ⋮ [n1] to enable jmalloc profiling: "export MALLOC_CONF=prof:true" or "ln -s prof:true /etc/malloc.conf"
I210118 05:38:18.749973 23 server/heapprofiler/statsprofiler.go:54 ⋮ [n1] writing memory stats to ‹/home/erikaosgue/erika_work/goland/truora_project/snake_project/backend/cockroach-data/logs/heap_profiler› at last every 1h0m0s
I210118 05:38:18.749998 23 server/server.go:1536 ⋮ [n1] starting http server at ‹127.0.0.1:8080› (use: ‹localhost:8080›)
I210118 05:38:18.750028 23 server/server.go:1543 ⋮ [n1] starting grpc/postgres server at ‹127.0.0.1:26257›
I210118 05:38:18.750053 23 server/server.go:1544 ⋮ [n1] advertising CockroachDB node at ‹localhost:26257›
I210118 05:38:18.763298 23 sql/sqlliveness/slinstance/slinstance.go:252 ⋮ [n1] starting SQL liveness instance
I210118 05:38:18.763659 23 server/server_sql.go:753 ⋮ [n1] done ensuring all necessary migrations have run
I210118 05:38:18.763686 23 server/server.go:1876 ⋮ [n1] serving sql connections
I210118 05:38:18.763923 23 cli/start.go:670 ⋮ [config] clusterID: ‹55e6c439-0264-4579-bb6d-df06d1461430›
I210118 05:38:18.763975 23 cli/start.go:680 ⋮ node startup completed:
CockroachDB node starting at 2021-01-18 05:38:18.763764381 +0000 UTC (took 0.1s)
build:               CCL v20.2.3 @ 2020/12/14 18:33:39 (go1.13.14)
webui:               ‹http://localhost:8080›
sql:                 ‹postgresql://root@localhost:26257?sslmode=disable›
RPC client flags:    ‹cockroach <client cmd> --host=localhost:26257 --insecure›
logs:                ‹/home/erikaosgue/erika_work/goland/truora_project/snake_project/backend/cockroach-data/logs›
temp dir:            ‹/home/erikaosgue/erika_work/goland/truora_project/snake_project/backend/cockroach-data/cockroach-temp315912015›
external I/O path:   ‹/home/erikaosgue/erika_work/goland/truora_project/snake_project/backend/cockroach-data/extern›
store[0]:            ‹path=/home/erikaosgue/erika_work/goland/truora_project/snake_project/backend/cockroach-data›
storage engine:      pebble
status:              restarted pre-existing node
clusterID:           ‹55e6c439-0264-4579-bb6d-df06d1461430›
nodeID:              1
I210118 05:38:18.764260 281 sql/temporary_schema.go:497 ⋮ [n1] running temporary object cleanup background job
I210118 05:38:18.764984 327 jobs/job_scheduler.go:346 ⋮ [n1] waiting 3m0s before scheduled jobs daemon start
I210118 05:38:18.791994 281 sql/temporary_schema.go:532 ⋮ [n1] found 0 temporary schemas
I210118 05:38:18.792047 281 sql/temporary_schema.go:535 ⋮ [n1] early exiting temporary schema cleaner as no temporary schemas were found
I210118 05:38:18.792065 281 sql/temporary_schema.go:536 ⋮ [n1] completed temporary object cleanup job
I210118 05:38:18.792081 281 sql/temporary_schema.go:614 ⋮ [n1] temporary object cleaner next scheduled to run at 2021-01-18 06:08:18.764214796 +0000 UTC
I210118 05:38:18.793835 328 server/server_update.go:55 ⋮ [n1] no need to upgrade, cluster already at the newest version
I210118 05:38:18.801136 279 sql/sqlliveness/slstorage/slstorage.go:342 ⋮ [n1] inserted sqlliveness session ‹3d7ae7b988ad4868a4dd30cf58f5edfe›
I210118 05:38:18.801190 279 sql/sqlliveness/slinstance/slinstance.go:143 ⋮ [n1] created new SQL liveness session ‹3d7ae7b988ad4868a4dd30cf58f5edfe›
I210118 05:38:18.811677 277 sql/event_log.go:162 ⋮ [n1] Event: ‹"node_restart"›, target: 1, info: ‹{Descriptor:{NodeID:1 Address:localhost:26257 Attrs: Locality: ServerVersion:20.2 BuildTag:v20.2.3 StartedAt:1610948298740105553 LocalityAddress:[] ClusterName: SQLAddress:localhost:26257} ClusterID:55e6c439-0264-4579-bb6d-df06d1461430 StartedAt:1610948298740105553 LastUp:1610948274111632565}›
I210118 05:38:20.644829 203 gossip/gossip.go:1508 ⋮ [n1] node has connected to cluster via gossip
I210118 05:38:20.645939 203 kv/kvserver/stores.go:255 ⋮ [n1] wrote 0 node addresses to persistent storage
I210118 05:38:28.751452 293 server/status/runtime.go:522 ⋮ [n1] runtime stats: 207 MiB RSS, 217 goroutines, 30 MiB/91 MiB/54 MiB GO alloc/idle/total, 24 MiB/64 MiB CGO alloc/total, 0.0 CGO/sec, 0.0/0.0 %(u/s)time, 0.0 %gc (121x), 1.5 MiB/90 KiB (r/w)net
I210118 05:38:38.495270 278 sql/sqlliveness/slstorage/slstorage.go:320 ⋮ [n1] deleted 1 expired SQL liveness sessions
I210118 05:38:38.753659 293 server/status/runtime.go:522 ⋮ [n1] runtime stats: 208 MiB RSS, 214 goroutines, 28 MiB/93 MiB/50 MiB GO alloc/idle/total, 24 MiB/64 MiB CGO alloc/total, 2.1 CGO/sec, 1.9/0.3 %(u/s)time, 0.0 %gc (1x), 72 KiB/63 KiB (r/w)net
I210118 05:38:38.757566 208 kv/kvserver/store.go:2638 ⋮ [n1,s1] sstables (read amplification = 1):
‹6 [ 27M 7 ]: 4M[6] 3M›
I210118 05:38:38.757850 208 kv/kvserver/store.go:2639 ⋮ [n1,s1] ‹›
‹__level_____count____size___score______in__ingest(sz_cnt)____move(sz_cnt)___write(sz_cnt)____read___r-amp___w-amp›
‹    WAL         2   561 K       -   558 K       -       -       -       -   561 K       -       -       -     1.0›
‹      0         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      1         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      2         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      3         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      4         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      5         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      6         7    27 M       -    90 K     0 B       0     0 B       0    27 M       7    27 M       1   303.7›
‹  total         7    27 M       -   561 K     0 B       0     0 B       0    27 M       7    27 M       1    50.0›
‹  flush         0›
‹compact         1     0 B          (size == estimated-debt)›
‹ memtbl         2   8.3 M›
‹zmemtbl         0     0 B›
‹   ztbl         0     0 B›
‹ bcache        25   776 K   32.1%  (score == hit-rate)›
‹ tcache         2   1.2 K   99.2%  (score == hit-rate)›
‹ titers         0›
‹ filter         -       -   88.8%  (score == utility)›
I210118 05:38:48.752402 293 server/status/runtime.go:522 ⋮ [n1] runtime stats: 208 MiB RSS, 214 goroutines, 23 MiB/98 MiB/51 MiB GO alloc/idle/total, 24 MiB/64 MiB CGO alloc/total, 13.6 CGO/sec, 1.9/0.5 %(u/s)time, 0.0 %gc (1x), 3.1 MiB/150 KiB (r/w)net
I210118 05:38:58.752516 293 server/status/runtime.go:522 ⋮ [n1] runtime stats: 209 MiB RSS, 215 goroutines, 31 MiB/90 MiB/51 MiB GO alloc/idle/total, 24 MiB/64 MiB CGO alloc/total, 0.5 CGO/sec, 1.8/0.6 %(u/s)time, 0.0 %gc (0x), 1.1 MiB/50 KiB (r/w)net
I210118 05:39:08.753745 293 server/status/runtime.go:522 ⋮ [n1] runtime stats: 209 MiB RSS, 215 goroutines, 26 MiB/94 MiB/53 MiB GO alloc/idle/total, 24 MiB/64 MiB CGO alloc/total, 0.1 CGO/sec, 2.3/0.4 %(u/s)time, 0.0 %gc (1x), 17 KiB/18 KiB (r/w)net
I210118 05:39:15.700002 1256 server/drain.go:74 ⋮ [n1] drain request received with doDrain = true, shutdown = false
I210118 05:39:15.703154 1256 server/drain.go:175 ⋮ [n1] drain remaining: 1
I210118 05:39:15.703197 1256 server/drain.go:177 ⋮ [n1] drain details: liveness record: 1
I210118 05:39:15.703271 1256 server/drain.go:99 ⋮ [n1] drain request completed without server shutdown
I210118 05:39:15.904602 1297 server/drain.go:74 ⋮ [n1] drain request received with doDrain = true, shutdown = false
I210118 05:39:15.911406 1297 server/drain.go:175 ⋮ [n1] drain remaining: 0
I210118 05:39:15.911597 1297 server/drain.go:99 ⋮ [n1] drain request completed without server shutdown
I210118 05:39:15.912706 1088 server/drain.go:74 ⋮ [n1] drain request received with doDrain = false, shutdown = true
I210118 05:39:15.913283 1089 util/stop/stopper.go:563 ⋮ [n1] quiescing
W210118 05:39:15.913460 279 sql/sqlliveness/slinstance/slinstance.go:182 ⋮ [n1] exiting heartbeat loop
W210118 05:39:15.913570 196 vendor/google.golang.org/grpc/internal/channelz/logging.go:73 ⋮ ‹grpc: addrConn.createTransport failed to connect to {localhost:26257  <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing loopback listener: mux: listener closed". Reconnecting...›
W210118 05:39:15.913767 322 jobs/registry.go:672 ⋮ canceling all adopted jobs due to stopper quiescing
