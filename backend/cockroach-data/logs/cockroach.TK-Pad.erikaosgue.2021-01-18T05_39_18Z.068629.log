I210118 05:39:18.283941 1 util/log/sync_buffer.go:195 ⋮ [config] file created at: 2021/01/18 05:39:18
I210118 05:39:18.283960 1 util/log/sync_buffer.go:195 ⋮ [config] running on machine: ‹TK-Pad›
I210118 05:39:18.283970 1 util/log/sync_buffer.go:195 ⋮ [config] binary: CockroachDB CCL v20.2.3 (x86_64-unknown-linux-gnu, built 2020/12/14 18:33:39, go1.13.14)
I210118 05:39:18.283978 1 util/log/sync_buffer.go:195 ⋮ [config] arguments: ‹[cockroach start-single-node --insecure --listen-addr=localhost:26257 --http-addr=localhost:8080]›
I210118 05:39:18.283996 1 util/log/sync_buffer.go:195 ⋮ [config] line format: [IWEF]yymmdd hh:mm:ss.uuuuuu goid file:line msg utf8=✓
W210118 05:39:18.283694 1 cli/start.go:1139 ⋮ ALL SECURITY CONTROLS HAVE BEEN DISABLED!

This mode is intended for non-production testing only.

In this mode:
- Your cluster is open to any client that can access ‹localhost›.
- Intruders with access to your machine or network can observe client-server traffic.
- Intruders can log in without password and read or write any data in the cluster.
- Intruders can consume all your server's resources and cause unavailability.
I210118 05:39:18.284198 1 cli/start.go:1149 ⋮ To start a secure server without mandating TLS for clients,
consider --accept-sql-without-tls instead. For other options, see:

- ‹https://go.crdb.dev/issue-v/53404/v20.2›
- https://www.cockroachlabs.com/docs/v20.2/secure-a-cluster.html
I210118 05:39:18.284774 1 server/status/recorder.go:605 ⋮ ‹available memory from cgroups is unsupported, using system memory 15 GiB instead: can't read available memory from cgroup v2 at /sys/fs/cgroup/unified/user.slice/user-1000.slice/user@1000.service/memory.max: open /sys/fs/cgroup/unified/user.slice/user-1000.slice/user@1000.service/memory.max: no such file or directory›
W210118 05:39:18.284813 1 cli/start.go:983 ⋮ ‹Using the default setting for --cache (128 MiB).›
‹  A significantly larger value is usually needed for good performance.›
‹  If you have a dedicated server a reasonable setting is --cache=.25 (3.8 GiB).›
I210118 05:39:18.285254 1 server/status/recorder.go:605 ⋮ ‹available memory from cgroups is unsupported, using system memory 15 GiB instead: can't read available memory from cgroup v2 at /sys/fs/cgroup/unified/user.slice/user-1000.slice/user@1000.service/memory.max: open /sys/fs/cgroup/unified/user.slice/user-1000.slice/user@1000.service/memory.max: no such file or directory›
I210118 05:39:18.285286 1 cli/start.go:1164 ⋮ ‹CockroachDB CCL v20.2.3 (x86_64-unknown-linux-gnu, built 2020/12/14 18:33:39, go1.13.14)›
I210118 05:39:18.286340 1 server/status/recorder.go:605 ⋮ ‹available memory from cgroups is unsupported, using system memory 15 GiB instead: can't read available memory from cgroup v2 at /sys/fs/cgroup/unified/user.slice/user-1000.slice/user@1000.service/memory.max: open /sys/fs/cgroup/unified/user.slice/user-1000.slice/user@1000.service/memory.max: no such file or directory›
I210118 05:39:18.286372 1 server/config.go:434 ⋮ system total memory: ‹15 GiB›
I210118 05:39:18.286389 1 server/config.go:436 ⋮ server configuration:
‹max offset             500000000›
‹cache size             128 MiB›
‹SQL memory pool size   3.8 GiB›
‹scan interval          10m0s›
‹scan min idle time     10ms›
‹scan max idle time     1s›
‹event log enabled      true›
I210118 05:39:18.286455 1 cli/start.go:961 ⋮ using local environment variables: ‹COCKROACH_BACKGROUND_RESTART=1›
I210118 05:39:18.286473 1 cli/start.go:968 ⋮ process identity: ‹uid 1000 euid 1000 gid 1000 egid 1000›
I210118 05:39:18.289453 1 cli/start.go:504 ⋮ GEOS loaded from directory ‹/usr/local/lib/cockroach›
I210118 05:39:18.289513 1 cli/start.go:509 ⋮ starting cockroach node
I210118 05:39:18.306450 71 server/server.go:782 ⋮ [n?] monitoring forward clock jumps based on server.clock.forward_jump_check_enabled
I210118 05:39:18.331140 71 server/config.go:625 ⋮ [n?] 1 storage engine‹› initialized
I210118 05:39:18.331163 71 server/config.go:628 ⋮ [n?] ‹Pebble cache size: 128 MiB›
I210118 05:39:18.331171 71 server/config.go:628 ⋮ [n?] ‹store 0: RocksDB, max size 0 B, max open file limit 10000›
I210118 05:39:18.333514 163 server/server.go:1416 ⋮ [n?] connecting to gossip network to verify cluster ID ‹"55e6c439-0264-4579-bb6d-df06d1461430"›
I210118 05:39:18.335397 71 gossip/gossip.go:403 ⋮ [n1] NodeDescriptor set to ‹node_id:1 address:<network_field:"tcp" address_field:"localhost:26257" > attrs:<> locality:<> ServerVersion:<major_val:20 minor_val:2 patch:0 unstable:0 > build_tag:"v20.2.3" started_at:1610948358335385566 cluster_name:"" sql_address:<network_field:"tcp" address_field:"localhost:26257" >›
I210118 05:39:18.343613 163 server/server.go:1419 ⋮ [n1] node connected via gossip
W210118 05:39:18.341747 273 kv/kvserver/replica_range_lease.go:555 ⋮ [n1,s1,r6/1:‹/Table/{SystemCon…-11}›] can't determine lease status of (n1,s1):1 due to node liveness error: node not in the liveness table
(1) attached stack trace
  -- stack trace:
  | github.com/cockroachdb/cockroach/pkg/kv/kvserver.init
  | 	/go/src/github.com/cockroachdb/cockroach/pkg/kv/kvserver/node_liveness.go:45
  | runtime.doInit
  | 	/usr/local/go/src/runtime/proc.go:5228
  | runtime.doInit
  | 	/usr/local/go/src/runtime/proc.go:5223
  | runtime.doInit
  | 	/usr/local/go/src/runtime/proc.go:5223
  | runtime.doInit
  | 	/usr/local/go/src/runtime/proc.go:5223
  | runtime.doInit
  | 	/usr/local/go/src/runtime/proc.go:5223
  | runtime.main
  | 	/usr/local/go/src/runtime/proc.go:190
  | runtime.goexit
  | 	/usr/local/go/src/runtime/asm_amd64.s:1357
Wraps: (2) node not in the liveness table
Error types: (1) *withstack.withStack (2) *errutil.leafError
W210118 05:39:18.343874 273 kv/kvserver/store.go:1704 ⋮ [n1,s1,r6/1:‹/Table/{SystemCon…-11}›] could not gossip system config: ‹[NotLeaseHolderError] r6: replica (n1,s1):1 not lease holder; lease holder unknown›
(1) ‹[NotLeaseHolderError] r6: replica (n1,s1):1 not lease holder; lease holder unknown›
Error types: (1) *roachpb.NotLeaseHolderError
I210118 05:39:18.345542 71 server/node.go:430 ⋮ [n1] initialized store [n1,s1]: disk (capacity=329 GiB, available=277 GiB, used=106 MiB, logicalBytes=209 MiB), ranges=36, leases=0, queries=0.00, writes=0.00, bytesPerReplica={p10=0.00 p25=0.00 p50=0.00 p75=23546.00 p90=50343.00 pMax=218631338.00}, writesPerReplica={p10=0.00 p25=0.00 p50=0.00 p75=0.00 p90=0.00 pMax=0.00}
I210118 05:39:18.345802 71 kv/kvserver/stores.go:236 ⋮ [n1] read 0 node addresses from persistent storage
I210118 05:39:18.347619 71 server/node.go:489 ⋮ [n1] started with engine type ‹2›
I210118 05:39:18.347913 71 server/node.go:491 ⋮ [n1] started with attributes ‹[]›
I210118 05:39:18.347972 71 server/goroutinedumper/goroutinedumper.go:120 ⋮ [n1] writing goroutine dumps to ‹/home/erikaosgue/erika_work/goland/truora_project/snake_project/backend/cockroach-data/logs/goroutine_dump›
I210118 05:39:18.347999 71 server/heapprofiler/heapprofiler.go:49 ⋮ [n1] writing go heap profiles to ‹/home/erikaosgue/erika_work/goland/truora_project/snake_project/backend/cockroach-data/logs/heap_profiler› at least every 1h0m0s
I210118 05:39:18.348023 71 server/heapprofiler/cgoprofiler.go:53 ⋮ [n1] to enable jmalloc profiling: "export MALLOC_CONF=prof:true" or "ln -s prof:true /etc/malloc.conf"
I210118 05:39:18.348038 71 server/heapprofiler/statsprofiler.go:54 ⋮ [n1] writing memory stats to ‹/home/erikaosgue/erika_work/goland/truora_project/snake_project/backend/cockroach-data/logs/heap_profiler› at last every 1h0m0s
I210118 05:39:18.348063 71 server/server.go:1536 ⋮ [n1] starting http server at ‹127.0.0.1:8080› (use: ‹localhost:8080›)
I210118 05:39:18.348094 71 server/server.go:1543 ⋮ [n1] starting grpc/postgres server at ‹127.0.0.1:26257›
I210118 05:39:18.348119 71 server/server.go:1544 ⋮ [n1] advertising CockroachDB node at ‹localhost:26257›
I210118 05:39:18.369579 71 sql/sqlliveness/slinstance/slinstance.go:252 ⋮ [n1] starting SQL liveness instance
I210118 05:39:18.370210 71 server/server_sql.go:753 ⋮ [n1] done ensuring all necessary migrations have run
I210118 05:39:18.370257 71 server/server.go:1876 ⋮ [n1] serving sql connections
I210118 05:39:18.370560 71 cli/start.go:670 ⋮ [config] clusterID: ‹55e6c439-0264-4579-bb6d-df06d1461430›
I210118 05:39:18.370671 71 cli/start.go:680 ⋮ node startup completed:
CockroachDB node starting at 2021-01-18 05:39:18.370365818 +0000 UTC (took 0.1s)
build:               CCL v20.2.3 @ 2020/12/14 18:33:39 (go1.13.14)
webui:               ‹http://localhost:8080›
sql:                 ‹postgresql://root@localhost:26257?sslmode=disable›
RPC client flags:    ‹cockroach <client cmd> --host=localhost:26257 --insecure›
logs:                ‹/home/erikaosgue/erika_work/goland/truora_project/snake_project/backend/cockroach-data/logs›
temp dir:            ‹/home/erikaosgue/erika_work/goland/truora_project/snake_project/backend/cockroach-data/cockroach-temp498114380›
external I/O path:   ‹/home/erikaosgue/erika_work/goland/truora_project/snake_project/backend/cockroach-data/extern›
store[0]:            ‹path=/home/erikaosgue/erika_work/goland/truora_project/snake_project/backend/cockroach-data›
storage engine:      pebble
status:              restarted pre-existing node
clusterID:           ‹55e6c439-0264-4579-bb6d-df06d1461430›
nodeID:              1
I210118 05:39:18.371016 333 jobs/job_scheduler.go:346 ⋮ [n1] waiting 3m0s before scheduled jobs daemon start
I210118 05:39:18.372234 191 sql/temporary_schema.go:497 ⋮ [n1] running temporary object cleanup background job
I210118 05:39:18.393234 191 sql/temporary_schema.go:532 ⋮ [n1] found 0 temporary schemas
I210118 05:39:18.393292 191 sql/temporary_schema.go:535 ⋮ [n1] early exiting temporary schema cleaner as no temporary schemas were found
I210118 05:39:18.393312 191 sql/temporary_schema.go:536 ⋮ [n1] completed temporary object cleanup job
I210118 05:39:18.393322 191 sql/temporary_schema.go:614 ⋮ [n1] temporary object cleaner next scheduled to run at 2021-01-18 06:09:18.370944534 +0000 UTC
I210118 05:39:18.400771 189 sql/sqlliveness/slstorage/slstorage.go:342 ⋮ [n1] inserted sqlliveness session ‹0f5378731069436389827b9149dd3047›
I210118 05:39:18.400871 189 sql/sqlliveness/slinstance/slinstance.go:143 ⋮ [n1] created new SQL liveness session ‹0f5378731069436389827b9149dd3047›
I210118 05:39:18.402372 334 server/server_update.go:55 ⋮ [n1] no need to upgrade, cluster already at the newest version
I210118 05:39:18.408363 187 sql/event_log.go:162 ⋮ [n1] Event: ‹"node_restart"›, target: 1, info: ‹{Descriptor:{NodeID:1 Address:localhost:26257 Attrs: Locality: ServerVersion:20.2 BuildTag:v20.2.3 StartedAt:1610948358335385566 LocalityAddress:[] ClusterName: SQLAddress:localhost:26257} ClusterID:55e6c439-0264-4579-bb6d-df06d1461430 StartedAt:1610948358335385566 LastUp:1610948355908916089}›
I210118 05:39:19.891168 166 gossip/gossip.go:1508 ⋮ [n1] node has connected to cluster via gossip
I210118 05:39:19.891934 166 kv/kvserver/stores.go:255 ⋮ [n1] wrote 0 node addresses to persistent storage
I210118 05:39:28.352432 201 server/status/runtime.go:522 ⋮ [n1] runtime stats: 186 MiB RSS, 205 goroutines, 16 MiB/40 MiB/42 MiB GO alloc/idle/total, 20 MiB/53 MiB CGO alloc/total, 0.0 CGO/sec, 0.0/0.0 %(u/s)time, 0.0 %gc (130x), 22 KiB/22 KiB (r/w)net
I210118 05:39:38.350817 201 server/status/runtime.go:522 ⋮ [n1] runtime stats: 186 MiB RSS, 205 goroutines, 16 MiB/40 MiB/42 MiB GO alloc/idle/total, 20 MiB/54 MiB CGO alloc/total, 2.1 CGO/sec, 1.8/0.5 %(u/s)time, 0.0 %gc (1x), 16 KiB/6.2 KiB (r/w)net
I210118 05:39:38.353163 307 kv/kvserver/store.go:2638 ⋮ [n1,s1] sstables (read amplification = 1):
‹6 [ 27M 7 ]: 4M[6] 3M›
I210118 05:39:38.353409 307 kv/kvserver/store.go:2639 ⋮ [n1,s1] ‹›
‹__level_____count____size___score______in__ingest(sz_cnt)____move(sz_cnt)___write(sz_cnt)____read___r-amp___w-amp›
‹    WAL         2   530 K       -   528 K       -       -       -       -   530 K       -       -       -     1.0›
‹      0         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      1         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      2         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      3         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      4         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      5         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      6         7    27 M       -   117 K     0 B       0     0 B       0    27 M       7    27 M       1   235.9›
‹  total         7    27 M       -   530 K     0 B       0     0 B       0    27 M       7    27 M       1    52.9›
‹  flush         0›
‹compact         1     0 B          (size == estimated-debt)›
‹ memtbl         2   4.3 M›
‹zmemtbl         0     0 B›
‹   ztbl         0     0 B›
‹ bcache        24   756 K   37.8%  (score == hit-rate)›
‹ tcache         2   1.2 K   99.4%  (score == hit-rate)›
‹ titers         0›
‹ filter         -       -   76.3%  (score == utility)›
I210118 05:39:48.352447 201 server/status/runtime.go:522 ⋮ [n1] runtime stats: 187 MiB RSS, 205 goroutines, 27 MiB/30 MiB/42 MiB GO alloc/idle/total, 20 MiB/54 MiB CGO alloc/total, 0.1 CGO/sec, 1.9/0.3 %(u/s)time, 0.0 %gc (0x), 22 KiB/17 KiB (r/w)net
I210118 05:39:48.538820 845 server/drain.go:74 ⋮ [n1] drain request received with doDrain = true, shutdown = false
I210118 05:39:48.542533 845 server/drain.go:175 ⋮ [n1] drain remaining: 1
I210118 05:39:48.542583 845 server/drain.go:177 ⋮ [n1] drain details: liveness record: 1
I210118 05:39:48.542691 845 server/drain.go:99 ⋮ [n1] drain request completed without server shutdown
I210118 05:39:48.743965 898 server/drain.go:74 ⋮ [n1] drain request received with doDrain = true, shutdown = false
I210118 05:39:48.750022 898 server/drain.go:175 ⋮ [n1] drain remaining: 0
I210118 05:39:48.750216 898 server/drain.go:99 ⋮ [n1] drain request completed without server shutdown
I210118 05:39:48.751521 814 server/drain.go:74 ⋮ [n1] drain request received with doDrain = false, shutdown = true
I210118 05:39:48.752093 815 util/stop/stopper.go:563 ⋮ [n1] quiescing
W210118 05:39:48.752462 189 sql/sqlliveness/slinstance/slinstance.go:182 ⋮ [n1] exiting heartbeat loop
W210118 05:39:48.752511 63 vendor/google.golang.org/grpc/internal/channelz/logging.go:73 ⋮ ‹grpc: addrConn.createTransport failed to connect to {localhost:26257  <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing loopback listener: mux: listener closed". Reconnecting...›
W210118 05:39:48.752615 328 jobs/registry.go:672 ⋮ canceling all adopted jobs due to stopper quiescing
